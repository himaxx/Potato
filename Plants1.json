{"id":"281516ad-f829-4eb1-b526-5b9a0472b928","data":{"nodes":[{"data":{"description":"Get chat inputs from the Playground.","display_name":"Chat Input","id":"ChatInput-Hxatv","node":{"base_classes":["Message"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Get chat inputs from the Playground.","display_name":"Chat Input","documentation":"","edited":false,"field_order":["input_value","should_store_message","sender","sender_name","session_id","files"],"frozen":false,"icon":"ChatInput","output_types":[],"outputs":[{"cache":true,"display_name":"Message","method":"message_response","name":"message","selected":"Message","types":["Message"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_NAME_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n"},"files":{"advanced":true,"display_name":"Files","dynamic":false,"fileTypes":["txt","md","mdx","csv","json","yaml","yml","xml","html","htm","pdf","docx","py","sh","sql","js","ts","tsx","jpg","jpeg","png","bmp","image"],"file_path":"","info":"Files to be sent with the message.","list":true,"name":"files","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"file","value":""},"input_value":{"advanced":false,"display_name":"Text","dynamic":false,"info":"Message to be passed as input.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"input_value","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"Mein ek kisan hun or meri aalo ki fasal me early blight bimari ho gayi hai kripya mujhe iska ilaaj batao. tum tables ke roop me jawab doge "},"sender":{"advanced":true,"display_name":"Sender Type","dynamic":false,"info":"Type of sender.","name":"sender","options":["Machine","User"],"placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"str","value":"User"},"sender_name":{"advanced":true,"display_name":"Sender Name","dynamic":false,"info":"Name of the sender.","input_types":["Message"],"list":false,"load_from_db":false,"name":"sender_name","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"User"},"session_id":{"advanced":true,"display_name":"Session ID","dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","input_types":["Message"],"list":false,"load_from_db":false,"name":"session_id","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"should_store_message":{"advanced":true,"display_name":"Store Messages","dynamic":false,"info":"Store the message in the history.","list":false,"name":"should_store_message","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":true}},"lf_version":"1.0.18"},"type":"ChatInput"},"dragging":false,"height":300,"id":"ChatInput-Hxatv","position":{"x":642.3545710150049,"y":220.22556606238678},"positionAbsolute":{"x":642.3545710150049,"y":220.22556606238678},"selected":false,"type":"genericNode","width":384},{"data":{"description":"Convert Data into plain text following a specified template.","display_name":"Parse Data","id":"ParseData-nAg78","node":{"base_classes":["Message"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Convert Data into plain text following a specified template.","display_name":"Parse Data","documentation":"","edited":false,"field_order":["data","template","sep"],"frozen":false,"icon":"braces","output_types":[],"outputs":[{"cache":true,"display_name":"Text","method":"parse_data","name":"text","selected":"Message","types":["Message"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DataInput, MultilineInput, Output, StrInput\nfrom langflow.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n"},"data":{"advanced":false,"display_name":"Data","dynamic":false,"info":"The data to convert to text.","input_types":["Data"],"list":false,"name":"data","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"other","value":""},"sep":{"advanced":true,"display_name":"Separator","dynamic":false,"info":"","list":false,"load_from_db":false,"name":"sep","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"str","value":"\n"},"template":{"advanced":false,"display_name":"Template","dynamic":false,"info":"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"template","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"{text}"}},"lf_version":"1.0.18"},"type":"ParseData"},"dragging":false,"height":376,"id":"ParseData-nAg78","position":{"x":1870.758124778794,"y":324.97868551639476},"positionAbsolute":{"x":1870.758124778794,"y":324.97868551639476},"selected":false,"type":"genericNode","width":384},{"data":{"description":"Create a prompt template with dynamic variables.","display_name":"Prompt","id":"Prompt-PvbAf","node":{"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"},"context":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"context","display_name":"context","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"question":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"question","display_name":"question","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"template":{"advanced":false,"display_name":"Template","dynamic":false,"info":"","list":false,"load_from_db":false,"name":"template","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"type":"prompt","value":"Context:\n\n{context}\n\nInstructions for the bot:\nAlways provide answers based on the given context.\nBased on the language the user ask questions you will respond in that language.\nStructure your answers clearly, with headings that are highlighted and slightly larger in size.\nWhere applicable, back up answers with examples and facts to make responses more helpful.\nWhenever needed, present information using tables to organize and clarify details.\nUse simple and easy-to-understand language.\nYou can respond in Hindi or English, depending on the userâ€™s preference.\nIf the context is unclear or missing, ask clarifying questions to provide the most accurate information.\nQuestion:\n\n{question}\n\nAnswer:"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["context","question"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false,"lf_version":"1.0.18"},"type":"Prompt"},"dragging":false,"height":499,"id":"Prompt-PvbAf","position":{"x":2486.0988668404975,"y":496.5120474157301},"positionAbsolute":{"x":2486.0988668404975,"y":496.5120474157301},"selected":false,"type":"genericNode","width":384},{"data":{"description":"Display a chat message in the Playground.","display_name":"Chat Output","id":"ChatOutput-RAuoW","node":{"base_classes":["Message"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Display a chat message in the Playground.","display_name":"Chat Output","documentation":"","edited":false,"field_order":["input_value","should_store_message","sender","sender_name","session_id","data_template"],"frozen":false,"icon":"ChatOutput","output_types":[],"outputs":[{"cache":true,"display_name":"Message","method":"message_response","name":"message","selected":"Message","types":["Message"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n"},"data_template":{"advanced":true,"display_name":"Data Template","dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","input_types":["Message"],"list":false,"load_from_db":false,"name":"data_template","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"{text}"},"input_value":{"advanced":false,"display_name":"Text","dynamic":false,"info":"Message to be passed as output.","input_types":["Message"],"list":false,"load_from_db":false,"name":"input_value","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"sender":{"advanced":true,"display_name":"Sender Type","dynamic":false,"info":"Type of sender.","name":"sender","options":["Machine","User"],"placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"str","value":"Machine"},"sender_name":{"advanced":true,"display_name":"Sender Name","dynamic":false,"info":"Name of the sender.","input_types":["Message"],"list":false,"load_from_db":false,"name":"sender_name","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"AI"},"session_id":{"advanced":true,"display_name":"Session ID","dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","input_types":["Message"],"list":false,"load_from_db":false,"name":"session_id","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"should_store_message":{"advanced":true,"display_name":"Store Messages","dynamic":false,"info":"Store the message in the history.","list":false,"name":"should_store_message","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":true}},"lf_version":"1.0.18"},"type":"ChatOutput"},"dragging":false,"height":300,"id":"ChatOutput-RAuoW","position":{"x":3769.242086248817,"y":585.3403837062634},"positionAbsolute":{"x":3769.242086248817,"y":585.3403837062634},"selected":false,"type":"genericNode","width":384},{"id":"FAISS-5cwwz","type":"genericNode","position":{"x":2880.6935584440475,"y":1445.665889232908},"data":{"type":"FAISS","node":{"template":{"_type":"Component","embedding":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"embedding","value":"","display_name":"Embedding","advanced":false,"input_types":["Embeddings"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"HandleInput"},"ingest_data":{"trace_as_metadata":true,"list":true,"trace_as_input":true,"required":false,"placeholder":"","show":true,"name":"ingest_data","value":"","display_name":"Ingest Data","advanced":false,"input_types":["Data"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"DataInput"},"allow_dangerous_deserialization":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"allow_dangerous_deserialization","value":true,"display_name":"Allow Dangerous Deserialization","advanced":true,"dynamic":false,"info":"Set to True to allow loading pickle files from untrusted sources. Only enable this if you trust the source of the data.","title_case":false,"type":"bool","_input_type":"BoolInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import List\n\nfrom langchain_community.vectorstores import FAISS\nfrom loguru import logger\n\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.io import BoolInput, DataInput, HandleInput, IntInput, MultilineInput, StrInput\nfrom langflow.schema import Data\n\n\nclass FaissVectorStoreComponent(LCVectorStoreComponent):\n    \"\"\"\n    FAISS Vector Store with search capabilities\n    \"\"\"\n\n    display_name: str = \"FAISS\"\n    description: str = \"FAISS Vector Store with search capabilities\"\n    documentation = \"https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/faiss\"\n    name = \"FAISS\"\n    icon = \"FAISS\"\n\n    inputs = [\n        StrInput(\n            name=\"index_name\",\n            display_name=\"Index Name\",\n            value=\"langflow_index\",\n        ),\n        StrInput(\n            name=\"persist_directory\",\n            display_name=\"Persist Directory\",\n            info=\"Path to save the FAISS index. It will be relative to where Langflow is running.\",\n        ),\n        MultilineInput(\n            name=\"search_query\",\n            display_name=\"Search Query\",\n        ),\n        DataInput(\n            name=\"ingest_data\",\n            display_name=\"Ingest Data\",\n            is_list=True,\n        ),\n        BoolInput(\n            name=\"allow_dangerous_deserialization\",\n            display_name=\"Allow Dangerous Deserialization\",\n            info=\"Set to True to allow loading pickle files from untrusted sources. Only enable this if you trust the source of the data.\",\n            advanced=True,\n            value=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            advanced=True,\n            value=4,\n        ),\n    ]\n\n    @check_cached_vector_store\n    def build_vector_store(self) -> FAISS:\n        \"\"\"\n        Builds the FAISS object.\n        \"\"\"\n        if not self.persist_directory:\n            raise ValueError(\"Folder path is required to save the FAISS index.\")\n        path = self.resolve_path(self.persist_directory)\n\n        documents = []\n\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n            else:\n                documents.append(_input)\n\n        faiss = FAISS.from_documents(documents=documents, embedding=self.embedding)\n        faiss.save_local(str(path), self.index_name)\n\n        return faiss\n\n    def search_documents(self) -> List[Data]:\n        \"\"\"\n        Search for documents in the FAISS vector store.\n        \"\"\"\n        if not self.persist_directory:\n            raise ValueError(\"Folder path is required to load the FAISS index.\")\n        path = self.resolve_path(self.persist_directory)\n\n        vector_store = FAISS.load_local(\n            folder_path=path,\n            embeddings=self.embedding,\n            index_name=self.index_name,\n            allow_dangerous_deserialization=self.allow_dangerous_deserialization,\n        )\n\n        if not vector_store:\n            raise ValueError(\"Failed to load the FAISS index.\")\n\n        logger.debug(f\"Search input: {self.search_query}\")\n        logger.debug(f\"Number of results: {self.number_of_results}\")\n\n        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():\n            docs = vector_store.similarity_search(\n                query=self.search_query,\n                k=self.number_of_results,\n            )\n\n            logger.debug(f\"Retrieved documents: {len(docs)}\")\n\n            data = docs_to_data(docs)\n            logger.debug(f\"Converted documents to data: {len(data)}\")\n            logger.debug(data)\n            return data  # Return the search results data\n        else:\n            logger.debug(\"No search input provided. Skipping search.\")\n            return []\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"index_name":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"index_name","value":"langflow_index","display_name":"Index Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"number_of_results":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"number_of_results","value":4,"display_name":"Number of Results","advanced":true,"dynamic":false,"info":"Number of results to return.","title_case":false,"type":"int","_input_type":"IntInput"},"persist_directory":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"persist_directory","value":"C:\\Users\\ha\\OneDrive\\Desktop\\GameAssets","display_name":"Persist Directory","advanced":false,"dynamic":false,"info":"Path to save the FAISS index. It will be relative to where Langflow is running.","title_case":false,"type":"str","_input_type":"StrInput"},"search_query":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"search_query","value":"","display_name":"Search Query","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"FAISS Vector Store with search capabilities","icon":"FAISS","base_classes":["Data","Retriever","VectorStore"],"display_name":"FAISS","documentation":"https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/faiss","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Retriever"],"selected":"Retriever","name":"base_retriever","display_name":"Retriever","method":"build_base_retriever","value":"__UNDEFINED__","cache":true},{"types":["Data"],"selected":"Data","name":"search_results","display_name":"Search Results","method":"search_documents","value":"__UNDEFINED__","cache":true},{"types":["VectorStore"],"selected":"VectorStore","name":"vector_store","display_name":"Vector Store","method":"cast_vector_store","value":"__UNDEFINED__","cache":true}],"field_order":["index_name","persist_directory","search_query","ingest_data","allow_dangerous_deserialization","embedding","number_of_results"],"beta":false,"edited":false,"lf_version":"1.0.18"},"id":"FAISS-5cwwz"},"selected":false,"width":384,"height":655,"positionAbsolute":{"x":2880.6935584440475,"y":1445.665889232908},"dragging":false},{"id":"FAISS-xmcca","type":"genericNode","position":{"x":1270.8647081009767,"y":306.2652518775307},"data":{"type":"FAISS","node":{"template":{"_type":"Component","embedding":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"embedding","value":"","display_name":"Embedding","advanced":false,"input_types":["Embeddings"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"HandleInput"},"ingest_data":{"trace_as_metadata":true,"list":true,"trace_as_input":true,"required":false,"placeholder":"","show":true,"name":"ingest_data","value":"","display_name":"Ingest Data","advanced":false,"input_types":["Data"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"DataInput"},"allow_dangerous_deserialization":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"allow_dangerous_deserialization","value":true,"display_name":"Allow Dangerous Deserialization","advanced":true,"dynamic":false,"info":"Set to True to allow loading pickle files from untrusted sources. Only enable this if you trust the source of the data.","title_case":false,"type":"bool","_input_type":"BoolInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import List\n\nfrom langchain_community.vectorstores import FAISS\nfrom loguru import logger\n\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.io import BoolInput, DataInput, HandleInput, IntInput, MultilineInput, StrInput\nfrom langflow.schema import Data\n\n\nclass FaissVectorStoreComponent(LCVectorStoreComponent):\n    \"\"\"\n    FAISS Vector Store with search capabilities\n    \"\"\"\n\n    display_name: str = \"FAISS\"\n    description: str = \"FAISS Vector Store with search capabilities\"\n    documentation = \"https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/faiss\"\n    name = \"FAISS\"\n    icon = \"FAISS\"\n\n    inputs = [\n        StrInput(\n            name=\"index_name\",\n            display_name=\"Index Name\",\n            value=\"langflow_index\",\n        ),\n        StrInput(\n            name=\"persist_directory\",\n            display_name=\"Persist Directory\",\n            info=\"Path to save the FAISS index. It will be relative to where Langflow is running.\",\n        ),\n        MultilineInput(\n            name=\"search_query\",\n            display_name=\"Search Query\",\n        ),\n        DataInput(\n            name=\"ingest_data\",\n            display_name=\"Ingest Data\",\n            is_list=True,\n        ),\n        BoolInput(\n            name=\"allow_dangerous_deserialization\",\n            display_name=\"Allow Dangerous Deserialization\",\n            info=\"Set to True to allow loading pickle files from untrusted sources. Only enable this if you trust the source of the data.\",\n            advanced=True,\n            value=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            advanced=True,\n            value=4,\n        ),\n    ]\n\n    @check_cached_vector_store\n    def build_vector_store(self) -> FAISS:\n        \"\"\"\n        Builds the FAISS object.\n        \"\"\"\n        if not self.persist_directory:\n            raise ValueError(\"Folder path is required to save the FAISS index.\")\n        path = self.resolve_path(self.persist_directory)\n\n        documents = []\n\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n            else:\n                documents.append(_input)\n\n        faiss = FAISS.from_documents(documents=documents, embedding=self.embedding)\n        faiss.save_local(str(path), self.index_name)\n\n        return faiss\n\n    def search_documents(self) -> List[Data]:\n        \"\"\"\n        Search for documents in the FAISS vector store.\n        \"\"\"\n        if not self.persist_directory:\n            raise ValueError(\"Folder path is required to load the FAISS index.\")\n        path = self.resolve_path(self.persist_directory)\n\n        vector_store = FAISS.load_local(\n            folder_path=path,\n            embeddings=self.embedding,\n            index_name=self.index_name,\n            allow_dangerous_deserialization=self.allow_dangerous_deserialization,\n        )\n\n        if not vector_store:\n            raise ValueError(\"Failed to load the FAISS index.\")\n\n        logger.debug(f\"Search input: {self.search_query}\")\n        logger.debug(f\"Number of results: {self.number_of_results}\")\n\n        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():\n            docs = vector_store.similarity_search(\n                query=self.search_query,\n                k=self.number_of_results,\n            )\n\n            logger.debug(f\"Retrieved documents: {len(docs)}\")\n\n            data = docs_to_data(docs)\n            logger.debug(f\"Converted documents to data: {len(data)}\")\n            logger.debug(data)\n            return data  # Return the search results data\n        else:\n            logger.debug(\"No search input provided. Skipping search.\")\n            return []\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"index_name":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"index_name","value":"langflow_index","display_name":"Index Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"number_of_results":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"number_of_results","value":4,"display_name":"Number of Results","advanced":true,"dynamic":false,"info":"Number of results to return.","title_case":false,"type":"int","_input_type":"IntInput"},"persist_directory":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"persist_directory","value":"C:\\Users\\ha\\OneDrive\\Desktop\\GameAssets","display_name":"Persist Directory","advanced":false,"dynamic":false,"info":"Path to save the FAISS index. It will be relative to where Langflow is running.","title_case":false,"type":"str","_input_type":"StrInput"},"search_query":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"search_query","value":"","display_name":"Search Query","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"FAISS Vector Store with search capabilities","icon":"FAISS","base_classes":["Data","Retriever","VectorStore"],"display_name":"FAISS","documentation":"https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/faiss","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Retriever"],"selected":"Retriever","name":"base_retriever","display_name":"Retriever","method":"build_base_retriever","value":"__UNDEFINED__","cache":true},{"types":["Data"],"selected":"Data","name":"search_results","display_name":"Search Results","method":"search_documents","value":"__UNDEFINED__","cache":true},{"types":["VectorStore"],"selected":"VectorStore","name":"vector_store","display_name":"Vector Store","method":"cast_vector_store","value":"__UNDEFINED__","cache":true}],"field_order":["index_name","persist_directory","search_query","ingest_data","allow_dangerous_deserialization","embedding","number_of_results"],"beta":false,"edited":false,"lf_version":"1.0.18"},"id":"FAISS-xmcca"},"selected":false,"width":384,"height":655,"positionAbsolute":{"x":1270.8647081009767,"y":306.2652518775307},"dragging":false},{"id":"CohereModel-msMgV","type":"genericNode","position":{"x":3142.1481665604424,"y":285.30194434070074},"data":{"type":"CohereModel","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_cohere import ChatCohere\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.io import FloatInput, SecretStrInput\n\n\nclass CohereComponent(LCModelComponent):\n    display_name = \"Cohere\"\n    description = \"Generate text using Cohere LLMs.\"\n    documentation = \"https://python.langchain.com/docs/modules/model_io/models/llms/integrations/cohere\"\n    icon = \"Cohere\"\n    name = \"CohereModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        SecretStrInput(\n            name=\"cohere_api_key\",\n            display_name=\"Cohere API Key\",\n            info=\"The Cohere API Key to use for the Cohere model.\",\n            advanced=False,\n            value=\"COHERE_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.75),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        cohere_api_key = self.cohere_api_key\n        temperature = self.temperature\n\n        if cohere_api_key:\n            api_key = SecretStr(cohere_api_key)\n        else:\n            api_key = None\n\n        output = ChatCohere(\n            temperature=temperature or 0.75,\n            cohere_api_key=api_key,\n        )\n\n        return output  # type: ignore\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"cohere_api_key":{"load_from_db":false,"required":false,"placeholder":"","show":true,"name":"cohere_api_key","value":"jVMoyMLTFvKiM1IitCgtEgcwWUqZ67tSPb8Da3uG","display_name":"Cohere API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The Cohere API Key to use for the Cohere model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":"0.5","display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generate text using Cohere LLMs.","icon":"Cohere","base_classes":["LanguageModel","Message"],"display_name":"Cohere","documentation":"https://python.langchain.com/docs/modules/model_io/models/llms/integrations/cohere","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","system_message","stream","cohere_api_key","temperature"],"beta":false,"edited":false,"lf_version":"1.0.18"},"id":"CohereModel-msMgV"},"selected":false,"width":384,"height":515,"positionAbsolute":{"x":3142.1481665604424,"y":285.30194434070074},"dragging":false},{"id":"OllamaEmbeddings-AgGA3","type":"genericNode","position":{"x":1634.3436047014081,"y":1984.4535426158966},"data":{"type":"OllamaEmbeddings","node":{"template":{"_type":"Component","base_url":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"base_url","value":"http://localhost:11434","display_name":"Ollama Base URL","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_community.embeddings import OllamaEmbeddings\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import FloatInput, MessageTextInput, Output\n\n\nclass OllamaEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Ollama Embeddings\"\n    description: str = \"Generate embeddings using Ollama models.\"\n    documentation = \"https://python.langchain.com/docs/integrations/text_embedding/ollama\"\n    icon = \"Ollama\"\n    name = \"OllamaEmbeddings\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"model\",\n            display_name=\"Ollama Model\",\n            value=\"llama3.1\",\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"Ollama Base URL\",\n            value=\"http://localhost:11434\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            output = OllamaEmbeddings(\n                model=self.model,\n                base_url=self.base_url,\n                temperature=self.temperature,\n            )  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not connect to Ollama API.\") from e\n        return output\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"model":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"model","value":"nomic-embed-text","display_name":"Ollama Model","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.1,"display_name":"Model Temperature","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generate embeddings using Ollama models.","icon":"Ollama","base_classes":["Embeddings"],"display_name":"Ollama Embeddings","documentation":"https://python.langchain.com/docs/integrations/text_embedding/ollama","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Embeddings"],"selected":"Embeddings","name":"embeddings","display_name":"Embeddings","method":"build_embeddings","value":"__UNDEFINED__","cache":true}],"field_order":["model","base_url","temperature"],"beta":false,"edited":false,"lf_version":"1.0.18"},"id":"OllamaEmbeddings-AgGA3"},"selected":false,"width":384,"height":394,"positionAbsolute":{"x":1634.3436047014081,"y":1984.4535426158966},"dragging":false},{"id":"Directory-9XDnJ","type":"genericNode","position":{"x":976.5168796514182,"y":1502.2341574377522},"data":{"type":"Directory","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import List\n\nfrom langflow.base.data.utils import parallel_load_data, parse_text_file_to_data, retrieve_file_paths\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, IntInput, MessageTextInput\nfrom langflow.schema import Data\nfrom langflow.template import Output\n\n\nclass DirectoryComponent(Component):\n    display_name = \"Directory\"\n    description = \"Recursively load files from a directory.\"\n    icon = \"folder\"\n    name = \"Directory\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"path\",\n            display_name=\"Path\",\n            info=\"Path to the directory to load files from.\",\n        ),\n        MessageTextInput(\n            name=\"types\",\n            display_name=\"Types\",\n            info=\"File types to load. Leave empty to load all types.\",\n            is_list=True,\n        ),\n        IntInput(\n            name=\"depth\",\n            display_name=\"Depth\",\n            info=\"Depth to search for files.\",\n            value=0,\n        ),\n        IntInput(\n            name=\"max_concurrency\",\n            display_name=\"Max Concurrency\",\n            advanced=True,\n            info=\"Maximum concurrency for loading files.\",\n            value=2,\n        ),\n        BoolInput(\n            name=\"load_hidden\",\n            display_name=\"Load Hidden\",\n            advanced=True,\n            info=\"If true, hidden files will be loaded.\",\n        ),\n        BoolInput(\n            name=\"recursive\",\n            display_name=\"Recursive\",\n            advanced=True,\n            info=\"If true, the search will be recursive.\",\n        ),\n        BoolInput(\n            name=\"silent_errors\",\n            display_name=\"Silent Errors\",\n            advanced=True,\n            info=\"If true, errors will not raise an exception.\",\n        ),\n        BoolInput(\n            name=\"use_multithreading\",\n            display_name=\"Use Multithreading\",\n            advanced=True,\n            info=\"If true, multithreading will be used.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"load_directory\"),\n    ]\n\n    def load_directory(self) -> List[Data]:\n        path = self.path\n        types = self.types or []  # self.types is already a list due to is_list=True\n        depth = self.depth\n        max_concurrency = self.max_concurrency\n        load_hidden = self.load_hidden\n        recursive = self.recursive\n        silent_errors = self.silent_errors\n        use_multithreading = self.use_multithreading\n\n        resolved_path = self.resolve_path(path)\n        file_paths = retrieve_file_paths(resolved_path, load_hidden, recursive, depth)\n\n        if types:\n            file_paths = [fp for fp in file_paths if any(fp.endswith(ext) for ext in types)]\n\n        loaded_data = []\n\n        if use_multithreading:\n            loaded_data = parallel_load_data(file_paths, silent_errors, max_concurrency)\n        else:\n            loaded_data = [parse_text_file_to_data(file_path, silent_errors) for file_path in file_paths]\n        loaded_data = list(filter(None, loaded_data))\n        self.status = loaded_data\n        return loaded_data  # type: ignore\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"depth":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"depth","value":0,"display_name":"Depth","advanced":false,"dynamic":false,"info":"Depth to search for files.","title_case":false,"type":"int","_input_type":"IntInput"},"load_hidden":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"load_hidden","value":false,"display_name":"Load Hidden","advanced":true,"dynamic":false,"info":"If true, hidden files will be loaded.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_concurrency":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"max_concurrency","value":2,"display_name":"Max Concurrency","advanced":true,"dynamic":false,"info":"Maximum concurrency for loading files.","title_case":false,"type":"int","_input_type":"IntInput"},"path":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"path","value":"C:\\Users\\ha\\OneDrive\\Desktop\\GameAssets\\Docs","display_name":"Path","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Path to the directory to load files from.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"recursive":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"recursive","value":false,"display_name":"Recursive","advanced":true,"dynamic":false,"info":"If true, the search will be recursive.","title_case":false,"type":"bool","_input_type":"BoolInput"},"silent_errors":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"silent_errors","value":false,"display_name":"Silent Errors","advanced":true,"dynamic":false,"info":"If true, errors will not raise an exception.","title_case":false,"type":"bool","_input_type":"BoolInput"},"types":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"name":"types","value":"","display_name":"Types","advanced":false,"input_types":["Message"],"dynamic":false,"info":"File types to load. Leave empty to load all types.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"use_multithreading":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"use_multithreading","value":false,"display_name":"Use Multithreading","advanced":true,"dynamic":false,"info":"If true, multithreading will be used.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Recursively load files from a directory.","icon":"folder","base_classes":["Data"],"display_name":"Directory","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"data","display_name":"Data","method":"load_directory","value":"__UNDEFINED__","cache":true}],"field_order":["path","types","depth","max_concurrency","load_hidden","recursive","silent_errors","use_multithreading"],"beta":false,"edited":false,"lf_version":"1.0.18"},"id":"Directory-9XDnJ"},"selected":false,"width":384,"height":471,"positionAbsolute":{"x":976.5168796514182,"y":1502.2341574377522},"dragging":false}],"edges":[{"className":"","data":{"sourceHandle":{"dataType":"ParseData","id":"ParseData-nAg78","name":"text","output_types":["Message"]},"targetHandle":{"fieldName":"context","id":"Prompt-PvbAf","inputTypes":["Message","Text"],"type":"str"}},"id":"reactflow__edge-ParseData-nAg78{Å“dataTypeÅ“:Å“ParseDataÅ“,Å“idÅ“:Å“ParseData-nAg78Å“,Å“nameÅ“:Å“textÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}-Prompt-PvbAf{Å“fieldNameÅ“:Å“contextÅ“,Å“idÅ“:Å“Prompt-PvbAfÅ“,Å“inputTypesÅ“:[Å“MessageÅ“,Å“TextÅ“],Å“typeÅ“:Å“strÅ“}","source":"ParseData-nAg78","sourceHandle":"{Å“dataTypeÅ“:Å“ParseDataÅ“,Å“idÅ“:Å“ParseData-nAg78Å“,Å“nameÅ“:Å“textÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}","target":"Prompt-PvbAf","targetHandle":"{Å“fieldNameÅ“:Å“contextÅ“,Å“idÅ“:Å“Prompt-PvbAfÅ“,Å“inputTypesÅ“:[Å“MessageÅ“,Å“TextÅ“],Å“typeÅ“:Å“strÅ“}"},{"className":"","data":{"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-Hxatv","name":"message","output_types":["Message"]},"targetHandle":{"fieldName":"question","id":"Prompt-PvbAf","inputTypes":["Message","Text"],"type":"str"}},"id":"reactflow__edge-ChatInput-Hxatv{Å“dataTypeÅ“:Å“ChatInputÅ“,Å“idÅ“:Å“ChatInput-HxatvÅ“,Å“nameÅ“:Å“messageÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}-Prompt-PvbAf{Å“fieldNameÅ“:Å“questionÅ“,Å“idÅ“:Å“Prompt-PvbAfÅ“,Å“inputTypesÅ“:[Å“MessageÅ“,Å“TextÅ“],Å“typeÅ“:Å“strÅ“}","source":"ChatInput-Hxatv","sourceHandle":"{Å“dataTypeÅ“:Å“ChatInputÅ“,Å“idÅ“:Å“ChatInput-HxatvÅ“,Å“nameÅ“:Å“messageÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}","target":"Prompt-PvbAf","targetHandle":"{Å“fieldNameÅ“:Å“questionÅ“,Å“idÅ“:Å“Prompt-PvbAfÅ“,Å“inputTypesÅ“:[Å“MessageÅ“,Å“TextÅ“],Å“typeÅ“:Å“strÅ“}"},{"source":"ChatInput-Hxatv","sourceHandle":"{Å“dataTypeÅ“:Å“ChatInputÅ“,Å“idÅ“:Å“ChatInput-HxatvÅ“,Å“nameÅ“:Å“messageÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}","target":"FAISS-xmcca","targetHandle":"{Å“fieldNameÅ“:Å“search_queryÅ“,Å“idÅ“:Å“FAISS-xmccaÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}","data":{"targetHandle":{"fieldName":"search_query","id":"FAISS-xmcca","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-Hxatv","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-Hxatv{Å“dataTypeÅ“:Å“ChatInputÅ“,Å“idÅ“:Å“ChatInput-HxatvÅ“,Å“nameÅ“:Å“messageÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}-FAISS-xmcca{Å“fieldNameÅ“:Å“search_queryÅ“,Å“idÅ“:Å“FAISS-xmccaÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}","selected":false,"className":""},{"source":"FAISS-xmcca","sourceHandle":"{Å“dataTypeÅ“:Å“FAISSÅ“,Å“idÅ“:Å“FAISS-xmccaÅ“,Å“nameÅ“:Å“search_resultsÅ“,Å“output_typesÅ“:[Å“DataÅ“]}","target":"ParseData-nAg78","targetHandle":"{Å“fieldNameÅ“:Å“dataÅ“,Å“idÅ“:Å“ParseData-nAg78Å“,Å“inputTypesÅ“:[Å“DataÅ“],Å“typeÅ“:Å“otherÅ“}","data":{"targetHandle":{"fieldName":"data","id":"ParseData-nAg78","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"FAISS","id":"FAISS-xmcca","name":"search_results","output_types":["Data"]}},"id":"reactflow__edge-FAISS-xmcca{Å“dataTypeÅ“:Å“FAISSÅ“,Å“idÅ“:Å“FAISS-xmccaÅ“,Å“nameÅ“:Å“search_resultsÅ“,Å“output_typesÅ“:[Å“DataÅ“]}-ParseData-nAg78{Å“fieldNameÅ“:Å“dataÅ“,Å“idÅ“:Å“ParseData-nAg78Å“,Å“inputTypesÅ“:[Å“DataÅ“],Å“typeÅ“:Å“otherÅ“}","className":""},{"source":"Prompt-PvbAf","sourceHandle":"{Å“dataTypeÅ“:Å“PromptÅ“,Å“idÅ“:Å“Prompt-PvbAfÅ“,Å“nameÅ“:Å“promptÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}","target":"CohereModel-msMgV","targetHandle":"{Å“fieldNameÅ“:Å“input_valueÅ“,Å“idÅ“:Å“CohereModel-msMgVÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}","data":{"targetHandle":{"fieldName":"input_value","id":"CohereModel-msMgV","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-PvbAf","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-PvbAf{Å“dataTypeÅ“:Å“PromptÅ“,Å“idÅ“:Å“Prompt-PvbAfÅ“,Å“nameÅ“:Å“promptÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}-CohereModel-msMgV{Å“fieldNameÅ“:Å“input_valueÅ“,Å“idÅ“:Å“CohereModel-msMgVÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}","className":""},{"source":"CohereModel-msMgV","sourceHandle":"{Å“dataTypeÅ“:Å“CohereModelÅ“,Å“idÅ“:Å“CohereModel-msMgVÅ“,Å“nameÅ“:Å“text_outputÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}","target":"ChatOutput-RAuoW","targetHandle":"{Å“fieldNameÅ“:Å“input_valueÅ“,Å“idÅ“:Å“ChatOutput-RAuoWÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-RAuoW","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"CohereModel","id":"CohereModel-msMgV","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-CohereModel-msMgV{Å“dataTypeÅ“:Å“CohereModelÅ“,Å“idÅ“:Å“CohereModel-msMgVÅ“,Å“nameÅ“:Å“text_outputÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}-ChatOutput-RAuoW{Å“fieldNameÅ“:Å“input_valueÅ“,Å“idÅ“:Å“ChatOutput-RAuoWÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}","className":""},{"source":"OllamaEmbeddings-AgGA3","sourceHandle":"{Å“dataTypeÅ“:Å“OllamaEmbeddingsÅ“,Å“idÅ“:Å“OllamaEmbeddings-AgGA3Å“,Å“nameÅ“:Å“embeddingsÅ“,Å“output_typesÅ“:[Å“EmbeddingsÅ“]}","target":"FAISS-5cwwz","targetHandle":"{Å“fieldNameÅ“:Å“embeddingÅ“,Å“idÅ“:Å“FAISS-5cwwzÅ“,Å“inputTypesÅ“:[Å“EmbeddingsÅ“],Å“typeÅ“:Å“otherÅ“}","data":{"targetHandle":{"fieldName":"embedding","id":"FAISS-5cwwz","inputTypes":["Embeddings"],"type":"other"},"sourceHandle":{"dataType":"OllamaEmbeddings","id":"OllamaEmbeddings-AgGA3","name":"embeddings","output_types":["Embeddings"]}},"id":"reactflow__edge-OllamaEmbeddings-AgGA3{Å“dataTypeÅ“:Å“OllamaEmbeddingsÅ“,Å“idÅ“:Å“OllamaEmbeddings-AgGA3Å“,Å“nameÅ“:Å“embeddingsÅ“,Å“output_typesÅ“:[Å“EmbeddingsÅ“]}-FAISS-5cwwz{Å“fieldNameÅ“:Å“embeddingÅ“,Å“idÅ“:Å“FAISS-5cwwzÅ“,Å“inputTypesÅ“:[Å“EmbeddingsÅ“],Å“typeÅ“:Å“otherÅ“}","className":""},{"source":"OllamaEmbeddings-AgGA3","sourceHandle":"{Å“dataTypeÅ“:Å“OllamaEmbeddingsÅ“,Å“idÅ“:Å“OllamaEmbeddings-AgGA3Å“,Å“nameÅ“:Å“embeddingsÅ“,Å“output_typesÅ“:[Å“EmbeddingsÅ“]}","target":"FAISS-xmcca","targetHandle":"{Å“fieldNameÅ“:Å“embeddingÅ“,Å“idÅ“:Å“FAISS-xmccaÅ“,Å“inputTypesÅ“:[Å“EmbeddingsÅ“],Å“typeÅ“:Å“otherÅ“}","data":{"targetHandle":{"fieldName":"embedding","id":"FAISS-xmcca","inputTypes":["Embeddings"],"type":"other"},"sourceHandle":{"dataType":"OllamaEmbeddings","id":"OllamaEmbeddings-AgGA3","name":"embeddings","output_types":["Embeddings"]}},"id":"reactflow__edge-OllamaEmbeddings-AgGA3{Å“dataTypeÅ“:Å“OllamaEmbeddingsÅ“,Å“idÅ“:Å“OllamaEmbeddings-AgGA3Å“,Å“nameÅ“:Å“embeddingsÅ“,Å“output_typesÅ“:[Å“EmbeddingsÅ“]}-FAISS-xmcca{Å“fieldNameÅ“:Å“embeddingÅ“,Å“idÅ“:Å“FAISS-xmccaÅ“,Å“inputTypesÅ“:[Å“EmbeddingsÅ“],Å“typeÅ“:Å“otherÅ“}","className":""},{"source":"Directory-9XDnJ","sourceHandle":"{Å“dataTypeÅ“:Å“DirectoryÅ“,Å“idÅ“:Å“Directory-9XDnJÅ“,Å“nameÅ“:Å“dataÅ“,Å“output_typesÅ“:[Å“DataÅ“]}","target":"FAISS-5cwwz","targetHandle":"{Å“fieldNameÅ“:Å“ingest_dataÅ“,Å“idÅ“:Å“FAISS-5cwwzÅ“,Å“inputTypesÅ“:[Å“DataÅ“],Å“typeÅ“:Å“otherÅ“}","data":{"targetHandle":{"fieldName":"ingest_data","id":"FAISS-5cwwz","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"Directory","id":"Directory-9XDnJ","name":"data","output_types":["Data"]}},"id":"reactflow__edge-Directory-9XDnJ{Å“dataTypeÅ“:Å“DirectoryÅ“,Å“idÅ“:Å“Directory-9XDnJÅ“,Å“nameÅ“:Å“dataÅ“,Å“output_typesÅ“:[Å“DataÅ“]}-FAISS-5cwwz{Å“fieldNameÅ“:Å“ingest_dataÅ“,Å“idÅ“:Å“FAISS-5cwwzÅ“,Å“inputTypesÅ“:[Å“DataÅ“],Å“typeÅ“:Å“otherÅ“}","className":""}],"viewport":{"x":-612.5470166140653,"y":-127.33459402313451,"zoom":0.4089461351089757}},"description":"Plants","name":"Plants1","last_tested_version":"1.0.18","endpoint_name":null,"is_component":false}